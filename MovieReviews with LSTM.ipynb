{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/sentimentanalysis/train.tsv\n",
      "/kaggle/input/sentimentanalysistest/test.tsv\n",
      "/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip\n",
      "/kaggle/input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv\n",
      "/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>3</td>\n",
       "      <td>, I suspect ,</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>3</td>\n",
       "      <td>I suspect ,</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>I suspect</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>I</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>suspect</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    PhraseId  SentenceId                                             Phrase  \\\n",
       "0          1           1  A series of escapades demonstrating the adage ...   \n",
       "1          2           1  A series of escapades demonstrating the adage ...   \n",
       "2          3           1                                           A series   \n",
       "3          4           1                                                  A   \n",
       "4          5           1                                             series   \n",
       "..       ...         ...                                                ...   \n",
       "95        96           3                                      , I suspect ,   \n",
       "96        97           3                                        I suspect ,   \n",
       "97        98           3                                          I suspect   \n",
       "98        99           3                                                  I   \n",
       "99       100           3                                            suspect   \n",
       "\n",
       "    Sentiment  \n",
       "0           1  \n",
       "1           2  \n",
       "2           2  \n",
       "3           2  \n",
       "4           2  \n",
       "..        ...  \n",
       "95          2  \n",
       "96          2  \n",
       "97          2  \n",
       "98          2  \n",
       "99          2  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/sentimentanalysis/train.tsv\",sep='\\t')\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Phrase[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A series of escapades demonstrating the adage that what is good for the goose'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Phrase[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fab6f07c3c8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Sentiment'].hist(bins=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data engineering\n",
    "### not much to do here : lemmatize, remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stopSet = set(stopwords.words(\"english\"))\n",
    "porter = nltk.PorterStemmer()\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "processed_reviews = []\n",
    "def process(phrase):\n",
    "    words_az=phrase.lower().split() # list\n",
    "    #words = [w for w in words_az if w not in stopSet]\n",
    "    words_lem=[WNlemma.lemmatize(t) for t in words_az if t not in string.punctuation ]\n",
    "    return \" \".join(words_lem)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['processed']=df[\"Phrase\"].apply(process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>a series of escapade demonstrating the adage t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>a series of escapade demonstrating the adage t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>a series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>series</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment                                          processed  \n",
       "0          1  a series of escapade demonstrating the adage t...  \n",
       "1          2  a series of escapade demonstrating the adage t...  \n",
       "2          2                                           a series  \n",
       "3          2                                                  a  \n",
       "4          2                                             series  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model : RFClassifier with CountVect or TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect = TfidfVectorizer()\n",
    "X = vect.fit_transform(df[\"processed\"])\n",
    "#X=X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,df[\"Sentiment\"],test_size=0.2,random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = RandomForestClassifier(n_estimators = 100)\n",
    "#model = MultinomialNB()\n",
    "model = model.fit( x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.36      0.40      1437\n",
      "           1       0.54      0.45      0.49      5606\n",
      "           2       0.69      0.82      0.75     15764\n",
      "           3       0.56      0.45      0.50      6592\n",
      "           4       0.50      0.38      0.43      1813\n",
      "\n",
      "    accuracy                           0.63     31212\n",
      "   macro avg       0.55      0.49      0.51     31212\n",
      "weighted avg       0.61      0.63      0.62     31212\n",
      "\n",
      "accuracy : 0.6279636037421504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score,f1_score,roc_auc_score\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "accuracy=accuracy_score(y_test,pred)\n",
    "#roc_auc=roc_auc_score(y_test,pred)\n",
    "print(classification_report(y_test,pred))\n",
    "print(\"accuracy :\",accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second try : TRY LSTM\n",
    "### Very useful alg for sequence modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def tokenize(phrase):\n",
    "    words = nltk.word_tokenize(phrase)\n",
    "    return words\n",
    "df['processed_tokens']=df[\"processed\"].apply(tokenize)\n",
    "\n",
    "target=df.Sentiment.values\n",
    "y_target=to_categorical(target)\n",
    "num_class=y_target.shape[1]\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(df[\"processed_tokens\"],y_target,test_size=0.2,random_state=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15183\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "# need to know the max size of the tokenized sentences for padding\n",
    "unique_set_of_words =  set()\n",
    "len_max = 0\n",
    "for i in range(df.processed_tokens.shape[0]) :\n",
    "    unique_set_of_words.update(df.processed_tokens[i])\n",
    "    if len_max < len(df.processed_tokens[i]):\n",
    "        len_max =len(df.processed_tokens[i])\n",
    "print(len(list(unique_set_of_words)))\n",
    "print(len_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KERAS Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124848, 49) (31212, 49)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=len(list(unique_words)))\n",
    "tokenizer.fit_on_texts(list(x_train))\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "#Padding\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=len_max)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=len_max)\n",
    "\n",
    "print(x_train.shape,x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model using Keras LSTM\n",
    "\n",
    "Multilayer Perceptron (MLP) for multi-class softmax classification:\n",
    "Let’s build what’s probably the most popular type of model in NLP at the moment: Long Short Term Memory network. \n",
    "\n",
    "This architecture is specially designed to work on sequence data.\n",
    "\n",
    "Don't have the ressources here to tune the model and increase accuracy. here is just an idea of model architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 49, 200)           3036600   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 49, 64)            67840     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 3,117,469\n",
      "Trainable params: 3,117,469\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "# Embedding layer : \n",
    "# vocabulary of len(list(unique_set_of_words)), size of the output vector 200, input_length =number of words in each phrase\n",
    "# \n",
    "model.add(Embedding(len(list(unique_set_of_words)),200,input_length=len_max))\n",
    "model.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=True))\n",
    "model.add(LSTM(32,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
    "model.add(Dense(16,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_class,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.005),metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/6\n",
      "124848/124848 [==============================] - 126s 1ms/step - loss: 1.0973 - accuracy: 0.5615 - val_loss: 0.8926 - val_accuracy: 0.6291\n",
      "Epoch 2/6\n",
      "124848/124848 [==============================] - 124s 997us/step - loss: 0.8932 - accuracy: 0.6368 - val_loss: 0.8465 - val_accuracy: 0.6432\n",
      "Epoch 3/6\n",
      "124848/124848 [==============================] - 123s 987us/step - loss: 0.8283 - accuracy: 0.6586 - val_loss: 0.8300 - val_accuracy: 0.6576\n",
      "Epoch 4/6\n",
      "124848/124848 [==============================] - 123s 988us/step - loss: 0.7947 - accuracy: 0.6689 - val_loss: 0.8325 - val_accuracy: 0.6621\n",
      "Epoch 5/6\n",
      "124848/124848 [==============================] - 124s 997us/step - loss: 0.7704 - accuracy: 0.6759 - val_loss: 0.8457 - val_accuracy: 0.6641\n",
      "Epoch 6/6\n",
      "124848/124848 [==============================] - 122s 979us/step - loss: 0.7507 - accuracy: 0.6843 - val_loss: 0.8502 - val_accuracy: 0.6644\n"
     ]
    }
   ],
   "source": [
    "classificator=model.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=6, batch_size=256, verbose=1\n",
    "                        #, callbacks=callback\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if early stopping is necessary \n",
    "### --> yes of course...\n",
    "#### Maybe try another learning rate, use cross-validation (gridsearch), or add a dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classificator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2f68e3a7deb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassificator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassificator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassificator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassificator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classificator' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(classificator.history['loss'])), classificator.history['loss'])\n",
    "plt.plot(range(len(classificator.history['loss'])), classificator.history['val_loss'])\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/kaggle/input/sentimentanalysis/sampleSubmission.csv' does not exist: b'/kaggle/input/sentimentanalysis/sampleSubmission.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-1e0ebce11791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/sentimentanalysis/sampleSubmission.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/kaggle/input/sentimentanalysis/sampleSubmission.csv' does not exist: b'/kaggle/input/sentimentanalysis/sampleSubmission.csv'"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('/kaggle/input/sentimentanalysistest/test.tsv',sep='\\t')\n",
    "\n",
    "test.processed = test[\"Phrase\"].apply(process) # lemmatize\n",
    "test.processed_tokens = test.processed.apply(tokenize) #tokenize with nltk\n",
    "X_test = tokenizer.texts_to_sequences(test.processed_tokens) # sequences\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=len_max) # padding for embedding\n",
    "\n",
    "prediction = model.predict_classes(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv',sep=',')\n",
    "sub.Sentiment=prediction\n",
    "sub.to_csv('Submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
