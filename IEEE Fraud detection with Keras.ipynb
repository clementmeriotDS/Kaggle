{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Fraud Detection in transactions with Keras","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Trying to build a suitable NN with Keras, dealing with the number of numerical and categorical variables \n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#  Libraries\nimport numpy as np \nimport pandas as pd \n# Data processing, metrics and modeling\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold,KFold\nfrom datetime import datetime\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nfrom sklearn import metrics\nfrom sklearn import preprocessing\n# Suppr warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport itertools\nfrom scipy import interp\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import rcParams","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Loading\nJust the standard loading of the data used in most other kernels. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\ntrain_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest_df = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\nprint(\"Train set shape : \"+str(train_df.shape))\nprint(\"Test set shape  : \"+str(test_df.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dropping TransactionDT (time) : we don't want to learn from this","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop([\"TransactionDT\"], axis = 1).reset_index()\ntest_df = test_df.drop([\"TransactionDT\"], axis = 1).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['nul'] = train_df.isna().sum(axis=1)\ntest_df['nul'] = test_df.isna().sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dropping very sparse categoricals features like deviceinfo and deviceid. We keep only ~55 : after some tests, a lot of variables cause rapid overfitting...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_ = train_df.iloc[:, :55]\ntest_df_ = test_df.iloc[:, :54]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_transaction, train_identity, test_transaction, test_identity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']\n#https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest_df-579654\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train_df[c + '_bin'] = train_df[c].map(emails)\n    test_df[c + '_bin'] = test_df[c].map(emails)\n    \n    train_df[c + '_suffix'] = train_df[c].map(lambda x: str(x).split('.')[-1])\n    test_df[c + '_suffix'] = test_df[c].map(lambda x: str(x).split('.')[-1])\n    \n    train_df[c + '_suffix'] = train_df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test_df[c + '_suffix'] = test_df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c1, c2 in train_df.dtypes.reset_index().values:\n    if c2=='O':\n        train_df[c1] = train_df[c1].map(lambda x: str(x).lower())\n        test_df[c1] = test_df[c1].map(lambda x: str(x).lower())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numerical and Categorical\nList the cat and num variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical = [\"TransactionAmt\", \"nulls1\", \"dist1\", \"dist2\"] + \\\n            [\"C\" + str(i) for i in range(1, 15)] + \\\n            [\"D\" + str(i) for i in range(1, 16)] + \\\n            [\"V\" + str(i) for i in range(1, 340)]\ncategorical = [\"ProductCD\", \"card1\", \"card2\", \"card3\", \"card4\", \"card5\", \"card6\", \"addr1\", \"addr2\",\n               \"P_emaildomain_bin\", \"P_emaildomain_suffix\", \"R_emaildomain_bin\", \"R_emaildomain_suffix\",\n               \"P_emaildomain\", \"R_emaildomain\",\n              \"DeviceInfo\", \"DeviceType\"] + [\"id_0\" + str(i) for i in range(1, 10)] +\\\n                [\"id_\" + str(i) for i in range(10, 39)] + \\\n                 [\"M\" + str(i) for i in range(1, 10)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical = [col for col in numerical if col in train_df.columns]\ncategorical = [col for col in categorical if col in train_df.columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Deal with NaN:\nWe could fill with the mean of the trainSet and then plug the train mean in the testSet, but here fore simplicity I fill with zeros","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def dealWithNan(df):\n    for x in list(df.columns.values):\n        if x in numerical:\n            #print(\"___________________\"+x)\n            #print(df[x].isna().sum())\n            df[x] = df[x].fillna(0)\n           #print(\"Mean-\"+str(df[x].mean()))\n    return df\ntrain_df=dealWithNan(train_df)\ntest_df=dealWithNan(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Label Encoding of the cat variables\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoding\ncategory_counts = {}\nfor f in categorical:\n    train_df[f] = train_df[f].replace(\"nan\", \"other\")\n    train_df[f] = train_df[f].replace(np.nan, \"other\")\n    test_df[f] = test_df[f].replace(\"nan\", \"other\")\n    test_df[f] = test_df[f].replace(np.nan, \"other\")\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n    train_df[f] = lbl.transform(list(train_df[f].values))\n    test_df[f] = lbl.transform(list(test_df[f].values))\n    category_counts[f] = len(list(lbl.classes_)) + 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numerical Scaling\n\nScaling of the numerical data , better with a NN. \nFirst we scale down the extreme positiv variables with a log1p transformation\n\nThen, standard scaler. Better for NN convergence","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in numerical:\n    scaler = StandardScaler()\n    if train_df[column].max() > 100 and train_df[column].min() >= 0:\n        train_df[column] = np.log1p(train_df[column])\n        test_df[column] = np.log1p(test_df[column])\n    scaler.fit(np.concatenate([train_df[column].values.reshape(-1,1), test_df[column].values.reshape(-1,1)]))\n    train_df[column] = scaler.transform(train_df[column].values.reshape(-1,1))\n    test_df[column] = scaler.transform(test_df[column].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'isFraud'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train Val Split","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tr_df, val_df = train_test_split(train_df, test_size = 0.2, random_state = 42, shuffle = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Grabbing the features we want to pass into the neural network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_input_features(df):\n    X = {'numerical':np.array(df[numerical])}\n    for cat in categorical:\n        X[cat] = np.array(df[cat])\n    return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Neural Network Model \n\nWe will use the embedding layer for categoricals and the numericals will go through feed forward dense layers. \n\nWe create our embedding layers such that we have as many rows as we had categories and the dimension of the embedding is the log1p + 1 of the number of categories : the log prevents to have to many dimensions for the categorical variables with a high cardinality.\n\nWe will then pass the embeddings through a spatial dropout layer which will drop dimensions within the embedding across batches and then flatten and concatenate. Then we will concatenate this to the numerical features and apply batch norm and then add some more dense layers after. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Concatenate, Input, Dense, Embedding, Flatten, Dropout, BatchNormalization, SpatialDropout1D\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import Model\nfrom keras.optimizers import  Adam\nimport keras.backend as k\ndef nn_model():\n    k.clear_session()\n\n    categorical_inputs = []\n    for cat in categorical:\n        categorical_inputs.append(Input(shape=[1], name=cat))\n\n    categorical_embeddings = []\n    for i, cat in enumerate(categorical):\n        categorical_embeddings.append(\n            Embedding(category_counts[cat], int(np.log1p(category_counts[cat]) + 1), name = cat + \"_embed\")(categorical_inputs[i]))\n\n    categorical_logits = Concatenate(name = \"categorical_conc\")([Flatten()(SpatialDropout1D(.1)(cat_emb)) for cat_emb in categorical_embeddings])\n#     categorical_logits = Dropout(.5)(categorical_logits)\n\n    numerical_inputs = Input(shape=[tr_df[numerical].shape[1]], name = 'numerical')\n    numerical_logits = Dropout(.1)(numerical_inputs)\n  \n    x = Concatenate()([\n        categorical_logits, \n        numerical_logits,\n    ])\n#     x = categorical_logits\n#     x = BatchNormalization()(x)\n    x = Dense(200, activation = 'relu')(x)\n    x = Dropout(.2)(x)\n    x = Dense(100, activation = 'relu')(x)\n    x = Dropout(.2)(x)\n    out = Dense(1, activation = 'sigmoid')(x)\n    \n\n    model = Model(inputs=categorical_inputs + [numerical_inputs],outputs=out)\n    loss = \"binary_crossentropy\"\n    model.compile(optimizer=Adam(lr = 0.01), loss = loss)\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from copy import deepcopy\ncategorical_save = deepcopy(categorical)\nnumerical_save = deepcopy(numerical)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Greedy Feature Selection\n\nFirst we will train the NN with all features in order to make a reference\n\nAfter that we greedily drop one feature at a time and see if it increases or decreases performance. If it increases upon dropping the feature then we will drop the feature. \n\nWe will iterate through epochs of the model and save the model weights if the score is an improvement upon previous best roc_auc_scores since this is competition metric. If the NN does not improve upon previous best after 4 epochs we will skip the rest of the training steps to save time. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#in case we want to restart the search. uncomment and rerun\n# categorical = deepcopy(categorical_save)\n# numerical = deepcopy(numerical_save)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = get_input_features(tr_df)\nX_valid = get_input_features(val_df)\nX_test = get_input_features(test_df)\ny_train = tr_df[target]\ny_valid = val_df[target]\nmodel = nn_model()\nbest_score = 0\npatience = 0\nfor i in range(100):\n    if patience < 3:\n        hist = model.fit(X_train, y_train, validation_data = (X_valid,y_valid), batch_size = 8000, epochs = 1, verbose = 1)\n        valid_preds = model.predict(X_valid, batch_size = 8000, verbose = True)\n        score = roc_auc_score(y_valid, valid_preds)\n        print(score)\n        if score > best_score:\n            model.save_weights(\"model.h5\")\n            best_score = score\n            patience = 0\n        else:\n            patience += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping categoricals","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cats = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for category_test in categorical_save:\n    categorical = [cat for cat in categorical_save if cat not in drop_cats]\n    categorical.remove(category_test)\n    print(categorical)\n    X_train = get_input_features(tr_df)\n    X_valid = get_input_features(val_df)\n    X_test = get_input_features(test_df)\n    y_train = tr_df[target]\n    y_valid = val_df[target]\n    model = nn_model()\n    local_score = 0\n    patience = 0\n    for i in range(100):\n        if patience < 3:\n            hist = model.fit(X_train, y_train, validation_data = (X_valid,y_valid), batch_size = 8000, epochs = 1, verbose = 0)\n            valid_preds = model.predict(X_valid, batch_size = 8000, verbose = False)\n            score = roc_auc_score(y_valid, valid_preds)\n            if score > local_score:\n                model.save_weights(\"model.h5\")\n                local_score = score\n                patience = 0\n            else:\n                patience += 1\n        else:\n            if local_score < best_score:\n                print(\"performance reduced when\", category_test, \"dropped to\", local_score, \"from\", best_score)\n                break\n            else:\n                drop_cats.append(category_test)\n                print(\"performance increased when\", category_test, \"dropped to\", local_score, \"from\", best_score)\n                best_score = local_score\n                model.save_weights(\"best_model.h5\")\n                break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = [cat for cat in categorical_save if cat not in drop_cats]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_nums = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping numeric","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for numerical_test in numerical_save:\n    numerical = [num for num in numerical_save if num not in drop_nums]\n    numerical.remove(numerical_test)\n    print(numerical)\n    X_train = get_input_features(tr_df)\n    X_valid = get_input_features(val_df)\n    X_test = get_input_features(test_df)\n    y_train = tr_df[target]\n    y_valid = val_df[target]\n    model = nn_model()\n    local_score = 0\n    patience = 0\n    for i in range(100):\n        if patience < 4:\n            hist = model.fit(X_train, y_train, validation_data = (X_valid,y_valid), batch_size = 8000, epochs = 1, verbose = 0)\n            valid_preds = model.predict(X_valid, batch_size = 8000, verbose = False)\n            score = roc_auc_score(y_valid, valid_preds)\n            if score > local_score:\n                model.save_weights(\"model.h5\")\n                local_score = score\n                patience = 0\n            else:\n                patience += 1\n        else:\n            if local_score < best_score:\n                print(\"performance reduced when\", numerical_test, \"dropped to\", local_score, \"from\", best_score)\n                break\n            else:\n                drop_nums.append(numerical_test)\n                print(\"performance increased when\", numerical_test, \"dropped to\", local_score, \"from\", best_score)\n                best_score = local_score\n                model.save_weights(\"best_model.h5\")\n                break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(drop_nums)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical = [num for num in numerical_save if num not in drop_nums]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = nn_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(\"best_model.h5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checks on validation set\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_valid = get_input_features(val_df)\nX_test = get_input_features(test_df)\nvalid_preds = model.predict(X_valid, batch_size = 500, verbose = True)\nscore = roc_auc_score(y_valid, valid_preds)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"fine fitting on validation data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit(X_valid,y_valid, batch_size = 8000, epochs = 3, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(X_test, batch_size = 2000, verbose = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(valid_preds).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(predictions).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['isFraud'] = predictions\nsample_submission.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}